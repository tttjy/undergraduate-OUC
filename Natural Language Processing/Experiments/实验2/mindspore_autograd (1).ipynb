{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自动微分\n",
    "\n",
    "[![在线运行](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_modelarts.png)](https://authoring-modelarts-cnnorth4.huaweicloud.com/console/lab?share-url-b64=aHR0cHM6Ly9vYnMuZHVhbHN0YWNrLmNuLW5vcnRoLTQubXlodWF3ZWljbG91ZC5jb20vbWluZHNwb3JlLXdlYnNpdGUvbm90ZWJvb2svcjEuNy90dXRvcmlhbHMvemhfY24vYmVnaW5uZXIvbWluZHNwb3JlX2F1dG9ncmFkLmlweW5i&imageid=9d63f4d1-dc09-4873-b669-3483cea777c0)&emsp;[![下载Notebook](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_notebook.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.7/tutorials/zh_cn/beginner/mindspore_autograd.ipynb)&emsp;[![下载样例代码](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_download_code.png)](https://obs.dualstack.cn-north-4.myhuaweicloud.com/mindspore-website/notebook/r1.7/tutorials/zh_cn/beginner/mindspore_autograd.py)&emsp;[![查看源文件](https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/website-images/r1.7/resource/_static/logo_source.png)](https://gitee.com/mindspore/docs/blob/r1.7/tutorials/source_zh_cn/beginner/autograd.ipynb)\n",
    "\n",
    "自动微分能够计算可导函数在某点处的导数值，是反向传播算法的一般化。自动微分主要解决的问题是将一个复杂的数学运算分解为一系列简单的基本运算，该功能对用户屏蔽了大量的求导细节和过程，大大降低了框架的使用门槛。\n",
    "\n",
    "MindSpore使用`ops.GradOperation`计算一阶导数，`ops.GradOperation`属性如下：\n",
    "\n",
    "+ `get_all`：计算梯度，如果等于False，获得第一个输入的梯度，如果等于True，获得所有输入的梯度。默认值为False。\n",
    "+ `get_by_list`：是否对权重参数进行求导，默认值为False。\n",
    "+ `sens_param`：是否对网络的输出值做缩放以改变最终梯度，默认值为False。\n",
    "\n",
    "本章使用MindSpore中的`ops.GradOperation`对函数 $f(x)=wx+b$ 求一阶导数。\n",
    "\n",
    "## 对输入求一阶导\n",
    "\n",
    "对输入求导前需要先定义公式：\n",
    "\n",
    "$$f(x)=wx+b \\tag {1} $$\n",
    "\n",
    "下面示例代码是公式(1)的表达，由于MindSpore采用函数式编程，因此所有计算公式表达都采用函数进行表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter, Tensor\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = Parameter(np.array([6.0]), name='w')\n",
    "        self.b = Parameter(np.array([1.0]), name='b')\n",
    "\n",
    "    def construct(self, x):\n",
    "        f = self.w * x + self.b\n",
    "        return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后定义求导类`GradNet`，类的`__init__`函数中定义需要求导的网络`self.net`和`ops.GradOperation`操作，类的`construct`函数中对`self.net`的输入进行求导。其对应MindSpore内部会产生如下公式(2)：\n",
    "\n",
    "$$f^{'}(x)=w\\tag {2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self, net):\n",
    "        super(GradNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.grad_op = ops.GradOperation()\n",
    "\n",
    "    def construct(self, x):\n",
    "        gradient_function = self.grad_op(self.net)\n",
    "        return gradient_function(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后定义权重参数为w，并对输入公式(1)中的输入参数x求一阶导数。从运行结果来看，公式(1)中的输入为6，即：\n",
    "\n",
    "$$f(x)=wx+b=6*x+1 \\tag {3}$$\n",
    "\n",
    "对上式进行求导，有：\n",
    "\n",
    "$$f^{'}(x)=w=6 \\tag {4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = Tensor([100,200],dtype=mstype.float32)\n",
    "output = GradNet(Net())(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MindSpore计算一阶导数方法`ops.GradOperation(get_all=False, get_by_list=False, sens_param=False)`，其中`get_all`为`False`时，只会对第一个输入求导，为`True`时，会对所有输入求导。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对权重求一阶导\n",
    "\n",
    "对权重参数求一阶导，需要将`ops.GradOperation`中的`get_by_list`设置为`True`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import ParameterTuple\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self, net):\n",
    "        super(GradNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.params = ParameterTuple(net.trainable_params())\n",
    "        self.grad_op = ops.GradOperation(get_by_list=True)  # 设置对权重参数进行一阶求导\n",
    "\n",
    "    def construct(self, x):\n",
    "        gradient_function = self.grad_op(self.net, self.params)\n",
    "        return gradient_function(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，对权重参数w进行求导：\n",
    "\n",
    "f(w) = wx+b\n",
    "\n",
    "f'(w) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1], dtype=Float64, value= [5.00000000e+000]), Tensor(shape=[1], dtype=Float64, value= [1.00000000e+000]))\n",
      "[5.]\n"
     ]
    }
   ],
   "source": [
    "# 对函数进行求导计算\n",
    "x = Tensor([5], dtype=mstype.float32)\n",
    "fx = GradNet(Net())(x)\n",
    "\n",
    "# 打印结果\n",
    "#print(f\"wgrad: {fx[0]}\\nbgrad: {fx[1]}\")\n",
    "print(fx)\n",
    "print(fx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若某些权重不需要进行求导，则在定义求导网络时，相应的权重参数声明定义的时候，将其属性`requires_grad`需设置为`False`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tensor(shape=[1], dtype=Float32, value= [5.00000000e+000]),)\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = Parameter(Tensor(np.array([6], np.float32)), name='w')\n",
    "        self.b = Parameter(Tensor(np.array([1.0], np.float32)), name='b', requires_grad=False)\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = x * self.w + self.b\n",
    "        return out\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self, net):\n",
    "        super(GradNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.params = ParameterTuple(net.trainable_params())\n",
    "        self.grad_op = ops.GradOperation(get_by_list=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        gradient_function = self.grad_op(self.net, self.params)\n",
    "        return gradient_function(x)\n",
    "\n",
    "# 构建求导网络\n",
    "x = Tensor([5], dtype=mstype.float32)\n",
    "fw = GradNet(Net())(x)\n",
    "\n",
    "print(fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度值缩放\n",
    "\n",
    "通过`sens_param`参数对网络的输出值做缩放以改变最终梯度。首先将`ops.GradOperation`中的`sens_param`设置为`True`，并确定缩放指数，其维度与输出维度保持一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Type Join Failed: dtype1 = Float32, dtype2 = Float64.\nFor more details, please refer to https://www.mindspore.cn/search?inputValue=Type%20Join%20Failed\n\nInner Message:\nThis: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), other: AbstractScalar(Type: Float64, Value: AnyValue, Shape: NoShape). Please check the node: @Net.28:29{[0]: 29, [1]: Net}\nThe function call stack:\nIn file C:\\Users\\15405\\AppData\\Local\\Temp\\ipykernel_19068\\3168270581.py:12/\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\core\\abstract\\abstract_value.cc:65 TypeJoinLogging\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gradient_function(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_wrt_output)\n\u001b[0;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m Tensor([\u001b[38;5;241m6\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mmstype\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m---> 15\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mGradNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(output)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:619\u001b[0m, in \u001b[0;36mCell.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_fn_registered():\n\u001b[0;32m    617\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCell\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms not support hook function in graph mode. If you want to use hook \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    618\u001b[0m                        \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction, please use context.set_context to set pynative mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 619\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m    622\u001b[0m \u001b[38;5;66;03m# Run in PyNative mode.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:1005\u001b[0m, in \u001b[0;36mCell.compile_and_run\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;124;03mCompile and run Cell, the input must be consistent with the input defined in construct.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;124;03m    Object, the result of executing.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_parallel_compile_and_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m-> 1005\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m new_inputs \u001b[38;5;241m=\u001b[39m _get_args_for_run(\u001b[38;5;28mself\u001b[39m, inputs)\n\u001b[0;32m   1008\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _cell_graph_executor(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mnew_inputs, phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphase)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mindspore\\lib\\site-packages\\mindspore\\nn\\cell.py:976\u001b[0m, in \u001b[0;36mCell.compile\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;124;03mCompile Cell as a computation graph, the input must be consistent with the input defined in construct.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;124;03m    inputs (tuple): Inputs of the Cell object.\u001b[39;00m\n\u001b[0;32m    974\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_shape_inputs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 976\u001b[0m     \u001b[43m_cell_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_parallel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_auto_parallel_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mjit_config_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jit_config_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_compile_dynamic_shape(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\mindspore\\lib\\site-packages\\mindspore\\common\\api.py:1131\u001b[0m, in \u001b[0;36m_CellGraphExecutor.compile\u001b[1;34m(self, obj, phase, do_convert, auto_parallel_mode, jit_config_dict, *args)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jit_config_dict:\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_executor\u001b[38;5;241m.\u001b[39mset_jit_config(jit_config_dict)\n\u001b[1;32m-> 1131\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_use_vm_mode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1132\u001b[0m obj\u001b[38;5;241m.\u001b[39mcompile_cache\u001b[38;5;241m.\u001b[39madd(phase)\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "\u001b[1;31mTypeError\u001b[0m: Type Join Failed: dtype1 = Float32, dtype2 = Float64.\nFor more details, please refer to https://www.mindspore.cn/search?inputValue=Type%20Join%20Failed\n\nInner Message:\nThis: AbstractScalar(Type: Float32, Value: AnyValue, Shape: NoShape), other: AbstractScalar(Type: Float64, Value: AnyValue, Shape: NoShape). Please check the node: @Net.28:29{[0]: 29, [1]: Net}\nThe function call stack:\nIn file C:\\Users\\15405\\AppData\\Local\\Temp\\ipykernel_19068\\3168270581.py:12/\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore\\core\\abstract\\abstract_value.cc:65 TypeJoinLogging\n"
     ]
    }
   ],
   "source": [
    "class GradNet(nn.Cell):\n",
    "    def __init__(self, net):\n",
    "        super(GradNet, self).__init__()\n",
    "        self.net = net\n",
    "        # 求导操作\n",
    "        self.grad_op = ops.GradOperation(sens_param=True)\n",
    "        # 缩放指数\n",
    "        self.grad_wrt_output = Tensor([2], dtype=mstype.float32)\n",
    "\n",
    "    def construct(self, x):\n",
    "        gradient_function = self.grad_op(self.net)\n",
    "        return gradient_function(x, self.grad_wrt_output)\n",
    "\n",
    "x = Tensor([6], dtype=mstype.float32)\n",
    "output = GradNet(Net())(x)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 停止计算梯度\n",
    "\n",
    "使用`ops.stop_gradient`可以停止计算梯度，示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wgrad: [0.]\n",
      "bgrad: [0.]\n"
     ]
    }
   ],
   "source": [
    "from mindspore.ops import stop_gradient\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.w = Parameter(Tensor(np.array([6], np.float32)), name='w')\n",
    "        self.b = Parameter(Tensor(np.array([1.0], np.float32)), name='b')\n",
    "\n",
    "    def construct(self, x):\n",
    "        out = x * self.w + self.b\n",
    "        # 停止梯度更新，out对梯度计算无贡献\n",
    "        out = stop_gradient(out)\n",
    "        return out\n",
    "\n",
    "class GradNet(nn.Cell):\n",
    "    def __init__(self, net):\n",
    "        super(GradNet, self).__init__()\n",
    "        self.net = net\n",
    "        self.params = ParameterTuple(net.trainable_params())\n",
    "        self.grad_op = ops.GradOperation(get_by_list=True)\n",
    "\n",
    "    def construct(self, x):\n",
    "        gradient_function = self.grad_op(self.net, self.params)\n",
    "        return gradient_function(x)\n",
    "\n",
    "x = Tensor([100], dtype=mstype.float32)\n",
    "output = GradNet(Net())(x)\n",
    "\n",
    "print(f\"wgrad: {output[0]}\\nbgrad: {output[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mindspore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
